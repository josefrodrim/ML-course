{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85112b4e",
   "metadata": {},
   "source": [
    "# Masterclass: Clasificación Supervisada (de 0 a producción)\n",
    "**Caso práctico:** Predicción de mora (mora_30d_next3m) usando `dataset_mora.csv`  \n",
    "**Autor:** Josef Renato Rodríguez Mallma  \n",
    "**Objetivo de la clase:** construir un pipeline completo de clasificación (EDA → baseline → modelos → métricas → umbral → interpretabilidad → checklist de producción)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f33fa3f",
   "metadata": {},
   "source": [
    "\n",
    "## Agenda de la masterclass\n",
    "1. ¿Qué es clasificación? (intuición + definición formal)\n",
    "2. Dataset de mora: carga, diccionario rápido y validación temporal\n",
    "3. Preparación de datos: tipos, nulos, outliers, encoding, escalado\n",
    "4. Split correcto (temporal) y por qué **NO** usar split aleatorio en riesgo\n",
    "5. Baseline: Logistic Regression (con regularización)\n",
    "6. Modelos no lineales: RandomForest / Gradient Boosting (sklearn)\n",
    "7. Métricas: Accuracy vs ROC-AUC vs PR-AUC, matriz de confusión, KS, lift\n",
    "8. Desbalance de clases: class_weight, thresholding, (y cuándo usar oversampling)\n",
    "9. Calibración y elección de umbral por costo\n",
    "10. Interpretabilidad: coeficientes (LR) + Permutation Importance\n",
    "11. Checklist final (anti-leakage, drift, PSI, monitoreo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e99f75e",
   "metadata": {},
   "source": [
    "\n",
    "## 1) ¿Qué es un problema de clasificación?\n",
    "En **clasificación** queremos predecir una **clase discreta**.  \n",
    "Ejemplos:\n",
    "- Fraude: {0,1}\n",
    "- Churn: {No, Sí}\n",
    "- Riesgo: {A,B,C,D} (multiclase)\n",
    "\n",
    "### 1.1 Definición formal\n",
    "Tenemos un conjunto de datos con:\n",
    "- Variables (features): \\(X = [x_1, x_2, ..., x_p]\\)\n",
    "- Etiqueta (target): \\(y\\)\n",
    "\n",
    "Buscamos una función \\(f(X)\\) que aproxime \\(P(y \\mid X)\\) o que asigne la clase más probable.\n",
    "\n",
    "### 1.2 En banca / riesgo\n",
    "Lo más común es:\n",
    "- **Binary classification**: default/no default, mora/no mora\n",
    "- Evaluar con métricas robustas a desbalance: **ROC-AUC, PR-AUC, KS, Lift**\n",
    "- Validación temporal para evitar **leakage** (fuga de información)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f6d85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Setup: imports y configuración\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report,\n",
    "    roc_auc_score, average_precision_score,\n",
    "    roc_curve, precision_recall_curve\n",
    ")\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b48a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Carga del dataset\n",
    "# Nota: asumimos que el archivo está en el mismo directorio del notebook o en una ruta conocida.\n",
    "# En este entorno (sandbox) está en /mnt/data/dataset_mora.csv\n",
    "\n",
    "DATA_PATH = \"/mnt/data/dataset_mora.csv\"\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf0e702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Revisión rápida de esquema\n",
    "display(df.dtypes)\n",
    "print(\"\\nNulos por columna:\")\n",
    "display(df.isna().sum().sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ba7e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\\\n",
    "# 5) Target y desbalance\n",
    "target = \"mora_30d_next3m\"\n",
    "\n",
    "# Aseguramos que el target sea binario (0/1)\n",
    "# Si ya viene como 0/1, esto no cambia nada. Si viniera como \"Sí/No\", lo mapeamos.\n",
    "if df[target].dtype == \"O\":\n",
    "    df[target] = df[target].map({\"No\": 0, \"Sí\": 1, \"Si\": 1, \"NO\": 0, \"SI\": 1}).astype(\"float\")\n",
    "\n",
    "# Distribución\n",
    "y = df[target].astype(int)\n",
    "counts = y.value_counts().sort_index()\n",
    "rate = counts.get(1, 0) / len(y)\n",
    "\n",
    "print(\"Distribución del target (0=no mora, 1=mora):\")\n",
    "display(counts)\n",
    "print(f\"Tasa de mora: {rate:.4f} ({rate*100:.2f}%)\")\n",
    "\n",
    "plt.figure()\n",
    "plt.bar([\"0\", \"1\"], [counts.get(0,0), counts.get(1,0)])\n",
    "plt.title(\"Distribución de clases (target)\")\n",
    "plt.xlabel(\"Clase\")\n",
    "plt.ylabel(\"N\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b4bd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Validación temporal (MUY importante en riesgo)\n",
    "# 'periodo' normalmente es YYYYMM. Verifiquemos y ordenemos.\n",
    "# La idea: entrenar con meses antiguos y testear con meses recientes.\n",
    "\n",
    "time_col = \"periodo\"\n",
    "\n",
    "# Intentamos convertir periodo a entero YYYYMM si es posible\n",
    "def to_yyyymm(x):\n",
    "    try:\n",
    "        # si viene como '202401' o 202401\n",
    "        return int(str(x)[:6])\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "df[time_col] = df[time_col].apply(to_yyyymm).astype(\"Int64\")\n",
    "\n",
    "print(\"periodo min/max:\", df[time_col].min(), df[time_col].max())\n",
    "display(df[[time_col]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e4ed51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Split temporal recomendado\n",
    "# Estrategia simple:\n",
    "# - Ordenamos por periodo\n",
    "# - Tomamos últimos periodos como test (holdout)\n",
    "# - El resto se divide en train/val (también temporal)\n",
    "\n",
    "df = df.sort_values(time_col).reset_index(drop=True)\n",
    "\n",
    "unique_periods = df[time_col].dropna().unique()\n",
    "unique_periods = np.sort(unique_periods)\n",
    "\n",
    "# Tomaremos ~15% de periodos como test (al final)\n",
    "n_test_periods = max(1, int(len(unique_periods) * 0.15))\n",
    "test_periods = unique_periods[-n_test_periods:]\n",
    "\n",
    "# Del restante, ~15% como validación (al final del train)\n",
    "train_periods = unique_periods[:-n_test_periods]\n",
    "n_val_periods = max(1, int(len(train_periods) * 0.15))\n",
    "val_periods = train_periods[-n_val_periods:]\n",
    "train_periods = train_periods[:-n_val_periods]\n",
    "\n",
    "train_df = df[df[time_col].isin(train_periods)].copy()\n",
    "val_df   = df[df[time_col].isin(val_periods)].copy()\n",
    "test_df  = df[df[time_col].isin(test_periods)].copy()\n",
    "\n",
    "print(\"Periodos -> train:\", train_periods[0], \"a\", train_periods[-1], \"(\", len(train_periods), \"meses )\")\n",
    "print(\"Periodos -> val  :\", val_periods[0], \"a\", val_periods[-1], \"(\", len(val_periods), \"meses )\")\n",
    "print(\"Periodos -> test :\", test_periods[0], \"a\", test_periods[-1], \"(\", len(test_periods), \"meses )\")\n",
    "\n",
    "print(\"\\nTamaños:\", len(train_df), len(val_df), len(test_df))\n",
    "\n",
    "def show_rate(name, d):\n",
    "    r = d[target].mean()\n",
    "    print(f\"{name}: mora rate = {r:.4f} ({r*100:.2f}%)\")\n",
    "\n",
    "show_rate(\"train\", train_df)\n",
    "show_rate(\"val\", val_df)\n",
    "show_rate(\"test\", test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40619a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Definimos features y tipos (numéricas vs categóricas)\n",
    "# IMPORTANTE: evitamos usar id_cliente como feature.\n",
    "# periodo puede usarse como feature (tendencia temporal) PERO cuidado: puede inducir leakage si el target cambia por políticas.\n",
    "# Para esta masterclass lo excluimos como feature y lo usamos solo para split.\n",
    "\n",
    "drop_cols = [\"id_cliente\", time_col, target]\n",
    "X_train = train_df.drop(columns=drop_cols, errors=\"ignore\")\n",
    "y_train = train_df[target].astype(int)\n",
    "\n",
    "X_val = val_df.drop(columns=drop_cols, errors=\"ignore\")\n",
    "y_val = val_df[target].astype(int)\n",
    "\n",
    "X_test = test_df.drop(columns=drop_cols, errors=\"ignore\")\n",
    "y_test = test_df[target].astype(int)\n",
    "\n",
    "# Detectamos columnas categóricas por dtype\n",
    "cat_cols = [c for c in X_train.columns if X_train[c].dtype == \"O\"]\n",
    "num_cols = [c for c in X_train.columns if c not in cat_cols]\n",
    "\n",
    "print(\"Num cols:\", num_cols)\n",
    "print(\"Cat cols:\", cat_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ba54a3",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Pipeline de preprocesamiento (industrial)\n",
    "Buenas prácticas:\n",
    "- **Imputación** de nulos: mediana (num), moda (cat)\n",
    "- **Encoding**: OneHot para categóricas\n",
    "- **Escalado**: StandardScaler solo para modelos sensibles a escala (Logistic, SVM, etc.)\n",
    "- Empaquetar todo en un **Pipeline** para evitar data leakage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed7680a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10) Construimos preprocesador con ColumnTransformer\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())  # útil para LogisticRegression\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, num_cols),\n",
    "        (\"cat\", categorical_transformer, cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "preprocess\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355120f3",
   "metadata": {},
   "source": [
    "\n",
    "## 11) Baseline: Logistic Regression (con regularización)\n",
    "**Por qué LR?**\n",
    "- Fuerte baseline en riesgo\n",
    "- Interpretable (coeficientes)\n",
    "- Rápido y estable\n",
    "- Produce probabilidades (scoring)\n",
    "\n",
    "**Regularización**\n",
    "- `penalty='l2'` (Ridge): reduce varianza, evita overfitting\n",
    "- `C`: inverso de la fuerza de regularización (más C = menos regularización)\n",
    "\n",
    "**Desbalance**\n",
    "- `class_weight='balanced'` ajusta el costo de errores según frecuencia de clases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4584dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12) Entrenamiento Logistic Regression (baseline)\n",
    "lr = LogisticRegression(\n",
    "    max_iter=200,\n",
    "    class_weight=\"balanced\",  # ayuda cuando la mora es baja\n",
    "    solver=\"lbfgs\"\n",
    ")\n",
    "\n",
    "lr_model = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", lr)\n",
    "])\n",
    "\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Probabilidades (score)\n",
    "val_proba = lr_model.predict_proba(X_val)[:, 1]\n",
    "test_proba = lr_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Métricas robustas a desbalance\n",
    "val_roc = roc_auc_score(y_val, val_proba)\n",
    "val_pr  = average_precision_score(y_val, val_proba)\n",
    "\n",
    "test_roc = roc_auc_score(y_test, test_proba)\n",
    "test_pr  = average_precision_score(y_test, test_proba)\n",
    "\n",
    "print(f\"[LR] VAL  ROC-AUC: {val_roc:.4f} | PR-AUC: {val_pr:.4f}\")\n",
    "print(f\"[LR] TEST ROC-AUC: {test_roc:.4f} | PR-AUC: {test_pr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228d2b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13) Curvas ROC y Precision-Recall\n",
    "def plot_roc_pr(y_true, proba, title_prefix=\"\"):\n",
    "    fpr, tpr, _ = roc_curve(y_true, proba)\n",
    "    prec, rec, _ = precision_recall_curve(y_true, proba)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.plot([0,1], [0,1], linestyle=\"--\")\n",
    "    plt.title(f\"{title_prefix} ROC Curve\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(rec, prec)\n",
    "    plt.title(f\"{title_prefix} Precision-Recall Curve\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.show()\n",
    "\n",
    "plot_roc_pr(y_val, val_proba, \"LR VAL\")\n",
    "plot_roc_pr(y_test, test_proba, \"LR TEST\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a746b36a",
   "metadata": {},
   "source": [
    "\n",
    "## 14) Umbral (threshold) y matriz de confusión\n",
    "Muchos modelos entregan **probabilidades**. Para decidir clase 0/1 se elige un umbral \\(t\\):\n",
    "- Si \\(p \\ge t\\) ⇒ predecir 1 (mora)\n",
    "- Si \\(p < t\\)  ⇒ predecir 0\n",
    "\n",
    "En riesgo, **no** siempre usamos \\(t=0.5\\).  \n",
    "Podemos optimizar por:\n",
    "- Recall mínimo (no dejar morosos fuera)\n",
    "- Precision mínimo (no castigar buenos clientes)\n",
    "- Costo financiero (FN y FP tienen costos diferentes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa56ce61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15) Selección de umbral por F1 o por Recall objetivo (ejemplo)\n",
    "def evaluate_threshold(y_true, proba, t):\n",
    "    pred = (proba >= t).astype(int)\n",
    "    cm = confusion_matrix(y_true, pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    precision = tp / (tp + fp + 1e-9)\n",
    "    recall    = tp / (tp + fn + 1e-9)\n",
    "    f1 = 2*precision*recall / (precision+recall+1e-9)\n",
    "    return precision, recall, f1, cm\n",
    "\n",
    "ts = np.linspace(0.05, 0.95, 19)\n",
    "rows = []\n",
    "for t in ts:\n",
    "    p,r,f1,_ = evaluate_threshold(y_val, val_proba, t)\n",
    "    rows.append((t,p,r,f1))\n",
    "\n",
    "thr_df = pd.DataFrame(rows, columns=[\"threshold\",\"precision\",\"recall\",\"f1\"])\n",
    "display(thr_df.sort_values(\"f1\", ascending=False).head(10))\n",
    "\n",
    "best_t = thr_df.loc[thr_df[\"f1\"].idxmax(), \"threshold\"]\n",
    "print(\"Best threshold (por F1 en VAL):\", best_t)\n",
    "\n",
    "p,r,f1,cm = evaluate_threshold(y_val, val_proba, best_t)\n",
    "print(f\"VAL @t={best_t:.2f} -> precision={p:.3f}, recall={r:.3f}, f1={f1:.3f}\")\n",
    "print(\"Confusion matrix [TN FP; FN TP]:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b1a40c",
   "metadata": {},
   "source": [
    "\n",
    "## 16) Modelos no lineales\n",
    "En datos reales, relaciones pueden ser no lineales (ej: utilización TC vs mora).\n",
    "Modelos típicos:\n",
    "- RandomForest (robusto, pero puede ser pesado)\n",
    "- Gradient Boosting (muy fuerte en tabular)\n",
    "- LightGBM/XGBoost/CatBoost (state-of-the-art, si están disponibles)\n",
    "\n",
    "Aquí usamos **HistGradientBoostingClassifier** (sklearn) que suele rendir muy bien.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb851aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17) RandomForest y HistGradientBoosting (sklearn)\n",
    "# Nota: Para árboles no necesitamos escalar, pero usamos el mismo preprocess (imputer + onehot)\n",
    "# Podemos crear un preprocess alternativo SIN scaler, pero lo mantenemos para simplicidad.\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=30,\n",
    "    n_jobs=-1,\n",
    "    class_weight=\"balanced_subsample\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf_model = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", rf)\n",
    "])\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_val_proba = rf_model.predict_proba(X_val)[:, 1]\n",
    "rf_test_proba = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(f\"[RF] VAL  ROC-AUC: {roc_auc_score(y_val, rf_val_proba):.4f} | PR-AUC: {average_precision_score(y_val, rf_val_proba):.4f}\")\n",
    "print(f\"[RF] TEST ROC-AUC: {roc_auc_score(y_test, rf_test_proba):.4f} | PR-AUC: {average_precision_score(y_test, rf_test_proba):.4f}\")\n",
    "\n",
    "hgb = HistGradientBoostingClassifier(\n",
    "    learning_rate=0.08,\n",
    "    max_depth=6,\n",
    "    max_iter=300,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "hgb_model = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", hgb)\n",
    "])\n",
    "\n",
    "hgb_model.fit(X_train, y_train)\n",
    "hgb_val_proba = hgb_model.predict_proba(X_val)[:, 1]\n",
    "hgb_test_proba = hgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(f\"[HGB] VAL  ROC-AUC: {roc_auc_score(y_val, hgb_val_proba):.4f} | PR-AUC: {average_precision_score(y_val, hgb_val_proba):.4f}\")\n",
    "print(f\"[HGB] TEST ROC-AUC: {roc_auc_score(y_test, hgb_test_proba):.4f} | PR-AUC: {average_precision_score(y_test, hgb_test_proba):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61d91c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18) Comparación consolidada\n",
    "models = [\n",
    "    (\"LogisticRegression\", val_proba, test_proba),\n",
    "    (\"RandomForest\", rf_val_proba, rf_test_proba),\n",
    "    (\"HistGradientBoosting\", hgb_val_proba, hgb_test_proba),\n",
    "]\n",
    "\n",
    "rows = []\n",
    "for name, vp, tp in models:\n",
    "    rows.append({\n",
    "        \"modelo\": name,\n",
    "        \"VAL_ROC_AUC\": roc_auc_score(y_val, vp),\n",
    "        \"VAL_PR_AUC\": average_precision_score(y_val, vp),\n",
    "        \"TEST_ROC_AUC\": roc_auc_score(y_test, tp),\n",
    "        \"TEST_PR_AUC\": average_precision_score(y_test, tp),\n",
    "    })\n",
    "\n",
    "comp = pd.DataFrame(rows).sort_values(\"TEST_ROC_AUC\", ascending=False)\n",
    "display(comp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c378028a",
   "metadata": {},
   "source": [
    "\n",
    "## 19) Interpretabilidad (sin SHAP)\n",
    "Si no queremos depender de librerías externas, podemos usar:\n",
    "- **Coeficientes** (Logistic Regression) para entender dirección e impacto\n",
    "- **Permutation Importance** para cualquier modelo (impacto en métrica al permutar una variable)\n",
    "\n",
    "> Nota: con OneHotEncoder, las features se expanden. Abajo mostramos cómo recuperar nombres.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1931b581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20) Recuperar nombres de features después del preprocess (OneHot)\n",
    "# Esto nos permite ver coeficientes o importancias por feature transformada.\n",
    "\n",
    "def get_feature_names(preprocessor, num_cols, cat_cols):\n",
    "    # num\n",
    "    num_features = num_cols\n",
    "    # cat\n",
    "    ohe = preprocessor.named_transformers_[\"cat\"].named_steps[\"onehot\"]\n",
    "    cat_features = ohe.get_feature_names_out(cat_cols).tolist()\n",
    "    return num_features + cat_features\n",
    "\n",
    "# Entrenamos el preprocesador del pipeline LR para poder extraer nombres\n",
    "pre = lr_model.named_steps[\"preprocess\"]\n",
    "pre.fit(X_train)\n",
    "\n",
    "feature_names = get_feature_names(pre, num_cols, cat_cols)\n",
    "print(\"N features after encoding:\", len(feature_names))\n",
    "feature_names[:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcddba98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 21) Coeficientes de Logistic Regression (top positivos y negativos)\n",
    "# Importante: coeficientes están en escala estandarizada por StandardScaler.\n",
    "\n",
    "lr_est = lr_model.named_steps[\"model\"]\n",
    "coefs = lr_est.coef_.ravel()\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    \"feature\": feature_names,\n",
    "    \"coef\": coefs\n",
    "}).sort_values(\"coef\", ascending=False)\n",
    "\n",
    "display(coef_df.head(15))\n",
    "display(coef_df.tail(15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211af7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 22) Permutation Importance (usando ROC-AUC en VALIDACIÓN)\n",
    "# OJO: permutation_importance requiere un estimator ya entrenado y datos transformables.\n",
    "# Usaremos el pipeline completo. Scoring='roc_auc'.\n",
    "\n",
    "result = permutation_importance(\n",
    "    hgb_model, X_val, y_val,\n",
    "    n_repeats=5,\n",
    "    random_state=42,\n",
    "    scoring=\"roc_auc\"\n",
    ")\n",
    "\n",
    "imp_df = pd.DataFrame({\n",
    "    \"feature\": X_val.columns,\n",
    "    \"importance_mean\": result.importances_mean,\n",
    "    \"importance_std\": result.importances_std\n",
    "}).sort_values(\"importance_mean\", ascending=False)\n",
    "\n",
    "display(imp_df.head(15))\n",
    "\n",
    "plt.figure()\n",
    "plt.barh(imp_df.head(15)[\"feature\"][::-1], imp_df.head(15)[\"importance_mean\"][::-1])\n",
    "plt.title(\"Top 15 Permutation Importance (HGB, ROC-AUC en VAL)\")\n",
    "plt.xlabel(\"Decrease in ROC-AUC (mean)\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aac0246",
   "metadata": {},
   "source": [
    "\n",
    "## 23) Checklist de producción (lo que haría un equipo de riesgo / MLOps)\n",
    "\n",
    "### Anti-leakage\n",
    "- Verificar que ninguna variable usa información del futuro (post-evento)\n",
    "- Validación temporal obligatoria\n",
    "\n",
    "### Estabilidad temporal / drift\n",
    "- PSI por variable (train vs. val/test vs. producción)\n",
    "- Monitoreo de tasa de mora por cohorte y segmento\n",
    "\n",
    "### Calibración y umbrales\n",
    "- Elegir umbral por costo (FN suele costar más que FP)\n",
    "- Calibrar probabilidades (Platt/Isotonic) si el score se usa como probabilidad real\n",
    "\n",
    "### Trazabilidad\n",
    "- Guardar: dataset version, features, hiperparámetros, métricas, fecha, código\n",
    "- Registrar decisioning: política de corte (threshold) y excepciones\n",
    "\n",
    "### Gobernanza\n",
    "- Explicabilidad mínima: top drivers por segmento\n",
    "- Documentación: diccionario + supuestos + limitaciones\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
