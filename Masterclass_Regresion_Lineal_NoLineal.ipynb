{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3c0887b",
   "metadata": {},
   "source": [
    "# Masterclass definitiva: Regresión lineal y no lineal  \n",
    "## Notebook paso a paso (teoría + ejemplos con números + gráficos + industria)\n",
    "\n",
    "**Objetivo:** que el alumno entienda cada concepto con esta secuencia fija:  \n",
    "**Definición real → Fórmula → Detalle (por qué) → Ejemplo con manzanitas → Ejemplo industrial → Importancia → Código + gráfico**\n",
    "\n",
    "> Recomendación docente: ejecuta celda por celda y pregunta “¿qué significa este número?” antes de seguir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651534a0",
   "metadata": {},
   "source": [
    "---\n",
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3474a926",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f537b5",
   "metadata": {},
   "source": [
    "---\n",
    "# PARTE 1 — REGRESIÓN LINEAL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdb52b7",
   "metadata": {},
   "source": [
    "## 1) Modelo lineal\n",
    "\n",
    "### 1️⃣ Definición real  \n",
    "Un **modelo lineal** predice un número (precio, ventas, salario, riesgo) como una **suma ponderada** de variables.\n",
    "\n",
    "### 2️⃣ Fórmula  \n",
    "\\[\n",
    "\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_p x_p\n",
    "\\]\n",
    "\n",
    "### 3️⃣ Detalle (qué significa cada término y por qué esa ecuación)\n",
    "- \\(\\hat{y}\\): predicción.  \n",
    "- \\(\\beta_0\\) (intercepto): valor “base” cuando todo \\(x_j = 0\\).  \n",
    "- \\(\\beta_j\\): **impacto marginal** (si \\(x_j\\) sube 1 unidad y lo demás se mantiene, \\(\\hat{y}\\) cambia \\(\\beta_j\\)).  \n",
    "- Se usa esta forma porque es **interpretable**: cada variable “explica” una parte del resultado.\n",
    "\n",
    "### 4️⃣ Ejemplo con manzanitas (números)  \n",
    "Salario (S/) según años de experiencia:  \n",
    "\\(\\hat{y} = 1200 + 500x\\).  \n",
    "Si \\(x=4\\) ⇒ \\(\\hat{y}=1200+2000=3200\\).\n",
    "\n",
    "### 5️⃣ Ejemplo industrial  \n",
    "Riesgo crediticio aproximado:  \n",
    "\\(\\widehat{Riesgo} = 0.02\\cdot Deuda - 0.01\\cdot Ingreso\\).\n",
    "\n",
    "### 6️⃣ Importancia  \n",
    "- Es el estándar cuando necesitas **explicabilidad** (banca, salud, auditoría).  \n",
    "- Es el punto de partida para casi todo lo demás (regularización, polinomios, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65247759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Código: ejemplo \"salario vs experiencia\" (fácil, con números entendibles)\n",
    "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)              # experiencia (años)\n",
    "y = np.array([1500, 2000, 2500, 3000, 3500])              # salario (S/.)\n",
    "\n",
    "lin = LinearRegression()\n",
    "lin.fit(X, y)\n",
    "\n",
    "b0 = lin.intercept_\n",
    "b1 = lin.coef_[0]\n",
    "\n",
    "print(\"Modelo aprendido: y_hat = b0 + b1*x\")\n",
    "print(\"b0 (intercepto):\", b0)\n",
    "print(\"b1 (pendiente):\", b1)\n",
    "\n",
    "# Predicción paso a paso para x=4\n",
    "x_new = np.array([[4]])\n",
    "y_hat = lin.predict(x_new)[0]\n",
    "print(\"\\nPredicción para 4 años:\", y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c711368b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Gráfico: puntos + recta\n",
    "y_pred = lin.predict(X)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X, y, alpha=0.8)\n",
    "plt.plot(X, y_pred)\n",
    "plt.xlabel(\"Experiencia (años)\")\n",
    "plt.ylabel(\"Salario (S/)\")\n",
    "plt.title(\"Regresión lineal: salario vs experiencia\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9450c53",
   "metadata": {},
   "source": [
    "---\n",
    "## 2) Función de costo (MSE): el “juez” del modelo\n",
    "\n",
    "### 1️⃣ Definición real  \n",
    "La **función de costo** mide *qué tan mal* está prediciendo el modelo.  \n",
    "Entrenar = **minimizar** ese costo.\n",
    "\n",
    "### 2️⃣ Fórmula (MSE / SSE normalizado)  \n",
    "\\[\n",
    "J(\\beta)=\\frac{1}{2n}\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2\n",
    "\\]\n",
    "\n",
    "### 3️⃣ Detalle (por qué esa ecuación)\n",
    "- \\(y_i - \\hat{y}_i\\): **residuo** (error) del punto \\(i\\).  \n",
    "- Se eleva al cuadrado para:  \n",
    "  1) evitar cancelación de signos,  \n",
    "  2) castigar fuerte errores grandes,  \n",
    "  3) hacer la función diferenciable y convexa (en regresión lineal).  \n",
    "- El factor \\(1/(2n)\\) simplifica derivadas y normaliza por tamaño del dataset.\n",
    "\n",
    "### 4️⃣ Ejemplo con manzanitas  \n",
    "Si real = 210 y pred = 200 → error = 10 → error² = 100.  \n",
    "Si error = 50 → error² = 2500 (mucho más castigo).\n",
    "\n",
    "### 5️⃣ Ejemplo industrial  \n",
    "En pricing: equivocarte por 1 sol en 1000 ventas es manejable; equivocarte por 50 soles en un segmento puede ser carísimo.  \n",
    "MSE empuja al modelo a evitar errores grandes.\n",
    "\n",
    "### 6️⃣ Importancia  \n",
    "Sin función de costo, el modelo no tiene “meta” y no puede aprender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea08e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Código: cálculo de errores y MSE sobre el ejemplo salario vs experiencia\n",
    "residuos = y - y_pred\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "mae = mean_absolute_error(y, y_pred)\n",
    "\n",
    "print(\"Residuos (y - y_hat):\", residuos)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"MSE:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa16bdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Gráfico: residuos (qué tan lejos está cada punto de la recta)\n",
    "plt.figure()\n",
    "plt.scatter(X, residuos, alpha=0.8)\n",
    "plt.axhline(0)\n",
    "plt.xlabel(\"Experiencia (años)\")\n",
    "plt.ylabel(\"Residuo (y - y_hat)\")\n",
    "plt.title(\"Residuos: idealmente alrededor de 0\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5894b425",
   "metadata": {},
   "source": [
    "---\n",
    "## 3) OLS (Ordinary Least Squares)\n",
    "\n",
    "### 1️⃣ Definición real  \n",
    "**OLS** (Mínimos Cuadrados Ordinarios) es el método que encuentra los \\(\\beta\\) que minimizan el MSE **sin penalización**.\n",
    "\n",
    "### 2️⃣ Fórmula (solución cerrada)  \n",
    "\\[\n",
    "\\beta = (X^TX)^{-1}X^Ty\n",
    "\\]\n",
    "\n",
    "### 3️⃣ Detalle (por qué importa)\n",
    "- Es “solución directa” (si la matriz se puede invertir).  \n",
    "- El problema: si hay **multicolinealidad** (variables muy correlacionadas), \\(X^TX\\) se vuelve inestable → coeficientes raros.\n",
    "\n",
    "### 4️⃣ Ejemplo con manzanitas  \n",
    "Si “edad” y “experiencia” son casi lo mismo, el modelo no sabe a cuál darle el efecto → coeficientes se vuelven inestables.\n",
    "\n",
    "### 5️⃣ Ejemplo industrial  \n",
    "En banca: ingreso, línea de crédito, gasto mensual suelen ir juntos (correlación alta). OLS puede “explotar”.\n",
    "\n",
    "### 6️⃣ Importancia  \n",
    "Es el baseline y el punto de comparación con Ridge/Lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7888617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Código: demostrar multicolinealidad con un dataset sintético fácil\n",
    "n = 200\n",
    "x1 = np.random.normal(size=n)\n",
    "x2 = x1 + np.random.normal(scale=0.05, size=n)  # casi igual a x1 (altamente correlacionada)\n",
    "X_mc = np.column_stack([x1, x2])\n",
    "\n",
    "# y depende realmente de x1\n",
    "y_mc = 5*x1 + np.random.normal(scale=1.0, size=n)\n",
    "\n",
    "ols_mc = LinearRegression()\n",
    "ols_mc.fit(X_mc, y_mc)\n",
    "\n",
    "print(\"Correlación entre x1 y x2:\", np.corrcoef(x1, x2)[0,1])\n",
    "print(\"Coeficientes OLS:\", ols_mc.coef_)\n",
    "print(\"Intercepto OLS:\", ols_mc.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664db926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Gráfico: correlación x1 vs x2 (visualiza multicolinealidad)\n",
    "plt.figure()\n",
    "plt.scatter(x1, x2, alpha=0.6)\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.title(\"Multicolinealidad: x2 ~ x1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f8d1e9",
   "metadata": {},
   "source": [
    "---\n",
    "## 4) Regularización (concepto general)\n",
    "\n",
    "### 1️⃣ Definición real  \n",
    "**Regularizar** es añadir una “regla” al modelo para evitar que se vuelva demasiado sensible a los datos (overfitting) y para estabilizar coeficientes.\n",
    "\n",
    "### 2️⃣ Fórmula general  \n",
    "\\[\n",
    "J(\\beta) = \\underbrace{\\frac{1}{2n}||y-X\\beta||^2}_{\\text{error}} + \\lambda\\cdot \\underbrace{\\Omega(\\beta)}_{\\text{penalización}}\n",
    "\\]\n",
    "\n",
    "### 3️⃣ Detalle\n",
    "- \\(\\Omega(\\beta)\\) define el tipo de regularización: L2 (Ridge), L1 (Lasso), etc.  \n",
    "- \\(\\lambda\\) controla “qué tan fuerte” castigas complejidad.  \n",
    "  - \\(\\lambda=0\\) → vuelves a OLS.  \n",
    "  - \\(\\lambda\\) grande → modelo más simple / más estable.\n",
    "\n",
    "### 4️⃣ Ejemplo con manzanitas  \n",
    "Es como decir: “Quiero predecir bien, pero **no te permito** usar coeficientes gigantes”.\n",
    "\n",
    "### 5️⃣ Importancia industrial  \n",
    "En producción, estabilidad > ajuste perfecto al entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ed21d0",
   "metadata": {},
   "source": [
    "---\n",
    "## 5) Ridge Regression (L2)\n",
    "\n",
    "### 1️⃣ Definición real  \n",
    "**Ridge** es OLS + castigo por coeficientes grandes. Reduce varianza y estabiliza cuando hay multicolinealidad.\n",
    "\n",
    "### 2️⃣ Fórmula  \n",
    "\\[\n",
    "J(\\beta)=\\frac{1}{2n}||y-X\\beta||^2 + \\lambda||\\beta||_2^2\n",
    "\\]\n",
    "\n",
    "### 3️⃣ Detalle (por qué L2)\n",
    "- L2 castiga “suavemente” valores grandes, encoge coeficientes.  \n",
    "- No los hace cero, solo más pequeños.  \n",
    "- Mantiene convexidad: mínimo global único.\n",
    "\n",
    "### 4️⃣ Ejemplo con manzanitas  \n",
    "Si OLS te da \\(\\beta=[10, -9]\\) (muy extremos), Ridge puede llevarlos a \\([6, -5]\\).\n",
    "\n",
    "### 5️⃣ Ejemplo industrial  \n",
    "Modelos de riesgo con cientos de variables correlacionadas: Ridge suele ser más estable y robusto.\n",
    "\n",
    "### 6️⃣ Importancia  \n",
    "Cuando la prioridad es **estabilidad y generalización**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa868ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Código: comparar OLS vs Ridge en el caso multicolineal (x1 ~ x2)\n",
    "ridge_mc = Ridge(alpha=10.0)   # alpha = lambda (en sklearn)\n",
    "ridge_mc.fit(X_mc, y_mc)\n",
    "\n",
    "print(\"Coeficientes OLS :\", ols_mc.coef_)\n",
    "print(\"Coeficientes Ridge:\", ridge_mc.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22efac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Gráfico: comparar coeficientes en barras (OLS vs Ridge)\n",
    "labels = [\"x1\", \"x2\"]\n",
    "ols_coef = ols_mc.coef_\n",
    "ridge_coef = ridge_mc.coef_\n",
    "\n",
    "xpos = np.arange(len(labels))\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(xpos - 0.15, ols_coef, width=0.3, label=\"OLS\")\n",
    "plt.bar(xpos + 0.15, ridge_coef, width=0.3, label=\"Ridge\")\n",
    "plt.xticks(xpos, labels)\n",
    "plt.title(\"Coeficientes: OLS vs Ridge (multicolinealidad)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1820895b",
   "metadata": {},
   "source": [
    "---\n",
    "## 6) Lasso Regression (L1)\n",
    "\n",
    "### 1️⃣ Definición real  \n",
    "**Lasso** es OLS + castigo L1. Además de estabilizar, puede hacer **selección de variables** porque empuja algunos coeficientes exactamente a cero.\n",
    "\n",
    "### 2️⃣ Fórmula  \n",
    "\\[\n",
    "J(\\beta)=\\frac{1}{2n}||y-X\\beta||^2 + \\lambda||\\beta||_1\n",
    "\\]\n",
    "\n",
    "### 3️⃣ Detalle (por qué L1)\n",
    "- \\(||\\beta||_1 = \\sum |\\beta_j|\\) genera geometría que favorece soluciones con ceros.  \n",
    "- Resultado: modelo más simple y “feature selection” automático.\n",
    "\n",
    "### 4️⃣ Ejemplo con manzanitas  \n",
    "Tienes 5 variables, pero solo 2 importan. Lasso tiende a dejar coeficientes ≈ 0 para las inútiles.\n",
    "\n",
    "### 5️⃣ Ejemplo industrial  \n",
    "En marketing: cientos de variables (campañas, clicks, horarios, segmentos). Lasso ayuda a quedarte con las que realmente aportan.\n",
    "\n",
    "### 6️⃣ Importancia  \n",
    "Reduce complejidad, facilita interpretación y baja costo de producción (menos features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93da4346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Código: dataset donde solo 2 variables importan y el resto es ruido\n",
    "n = 300\n",
    "p = 8\n",
    "X_fs = np.random.normal(size=(n, p))\n",
    "true_beta = np.array([4.0, -3.0] + [0.0]*(p-2))     # solo x0 y x1 importan\n",
    "y_fs = X_fs @ true_beta + np.random.normal(scale=1.0, size=n)\n",
    "\n",
    "ols_fs = LinearRegression().fit(X_fs, y_fs)\n",
    "ridge_fs = Ridge(alpha=10.0).fit(X_fs, y_fs)\n",
    "lasso_fs = Lasso(alpha=0.1, max_iter=10000).fit(X_fs, y_fs)\n",
    "\n",
    "print(\"True beta :\", true_beta)\n",
    "print(\"OLS beta  :\", np.round(ols_fs.coef_, 3))\n",
    "print(\"Ridge beta:\", np.round(ridge_fs.coef_, 3))\n",
    "print(\"Lasso beta:\", np.round(lasso_fs.coef_, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80a7193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Gráfico: coeficientes (ver ceros en Lasso)\n",
    "plt.figure()\n",
    "plt.plot(true_beta, marker=\"o\", label=\"True beta\")\n",
    "plt.plot(ols_fs.coef_, marker=\"o\", label=\"OLS\")\n",
    "plt.plot(ridge_fs.coef_, marker=\"o\", label=\"Ridge\")\n",
    "plt.plot(lasso_fs.coef_, marker=\"o\", label=\"Lasso\")\n",
    "plt.title(\"Comparación de coeficientes: OLS vs Ridge vs Lasso\")\n",
    "plt.xlabel(\"Índice de variable\")\n",
    "plt.ylabel(\"Coeficiente\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ea052b",
   "metadata": {},
   "source": [
    "---\n",
    "# PARTE 2 — REGRESIÓN NO LINEAL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42408541",
   "metadata": {},
   "source": [
    "## 7) Regresión polinomial\n",
    "\n",
    "### 1️⃣ Definición real  \n",
    "Se usa cuando la relación no es una recta. Se agregan términos como \\(x^2, x^3\\), etc., para capturar curvatura.\n",
    "\n",
    "### 2️⃣ Fórmula  \n",
    "\\[\n",
    "\\hat{y}=\\beta_0+\\beta_1x+\\beta_2x^2+\\dots+\\beta_nx^n\n",
    "\\]\n",
    "\n",
    "### 3️⃣ Detalle (por qué se sigue llamando “lineal”)  \n",
    "Porque es lineal en los coeficientes \\(\\beta\\). El modelo sigue siendo una combinación lineal de parámetros.\n",
    "\n",
    "### 4️⃣ Ejemplo con manzanitas  \n",
    "Fertilizante vs cosecha:  \n",
    "- poco fertilizante → baja cosecha  \n",
    "- óptimo → máxima cosecha  \n",
    "- demasiado → baja cosecha  \n",
    "Esto es forma “U” (cuadrática).\n",
    "\n",
    "### 5️⃣ Industria  \n",
    "Economía (rendimientos decrecientes), ingeniería, fenómenos físicos.\n",
    "\n",
    "### 6️⃣ Importancia  \n",
    "Primer salto desde “recta” hacia “curvas” sin perder interpretabilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe5c03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Código + gráfico: datos con forma de U (cuadrática) y comparación lineal vs polinomial\n",
    "X_u = np.linspace(-3, 3, 200).reshape(-1, 1)\n",
    "y_u = (X_u[:,0]**2) + np.random.normal(scale=0.6, size=len(X_u))  # U + ruido\n",
    "\n",
    "# Modelo lineal (falla)\n",
    "lin_u = LinearRegression().fit(X_u, y_u)\n",
    "y_lin_u = lin_u.predict(X_u)\n",
    "\n",
    "# Modelo polinomial grado 2 (captura)\n",
    "poly2 = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_u_poly = poly2.fit_transform(X_u)      # [x, x^2]\n",
    "poly_u = LinearRegression().fit(X_u_poly, y_u)\n",
    "y_poly_u = poly_u.predict(X_u_poly)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X_u, y_u, alpha=0.4, label=\"Datos\")\n",
    "plt.plot(X_u, y_lin_u, label=\"Lineal\")\n",
    "plt.plot(X_u, y_poly_u, label=\"Polinomial grado 2\")\n",
    "plt.title(\"Lineal vs Polinomial (forma U)\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"R2 lineal:\", r2_score(y_u, y_lin_u))\n",
    "print(\"R2 poli2 :\", r2_score(y_u, y_poly_u))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dd6c9f",
   "metadata": {},
   "source": [
    "---\n",
    "## 8) Modelos intrínsecamente no lineales (log / exp)\n",
    "\n",
    "### 1️⃣ Definición real  \n",
    "Son relaciones donde la forma funcional incluye logaritmos o exponenciales. Se usan cuando los incrementos tienen rendimientos decrecientes (log) o crecimiento acelerado (exp).\n",
    "\n",
    "### 2️⃣ Fórmulas típicas\n",
    "- Logarítmica: \\(\\hat{y}=\\beta_0+\\beta_1\\ln(x)\\)  \n",
    "- Exponencial: \\(\\hat{y}=\\beta_0 e^{\\beta_1 x}\\)\n",
    "\n",
    "### 3️⃣ Detalle\n",
    "- **Log**: los primeros aumentos importan mucho, luego “se aplana”.  \n",
    "- **Exp**: crece rápido (viralidad, intereses compuestos, epidemias).\n",
    "\n",
    "### 4️⃣ Ejemplos con manzanitas\n",
    "- Log: satisfacción vs salario (subir de 1000 a 2000 impacta más que de 9000 a 10000).  \n",
    "- Exp: usuarios virales (cada usuario trae más usuarios).\n",
    "\n",
    "### 5️⃣ Industria\n",
    "Finanzas (interés compuesto), epidemiología, crecimiento de usuarios, decaimiento físico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed98b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Código + gráfico: ejemplo logarítmico (rendimientos decrecientes)\n",
    "x = np.linspace(1, 100, 200)\n",
    "y_log = 10 + 5*np.log(x) + np.random.normal(scale=0.5, size=len(x))\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(x, y_log, alpha=0.4)\n",
    "plt.title(\"Ejemplo logarítmico: rendimientos decrecientes\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93898c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Código + gráfico: ejemplo exponencial (crecimiento acelerado)\n",
    "x = np.linspace(0, 3, 200)\n",
    "y_exp = 2*np.exp(1.2*x) + np.random.normal(scale=0.8, size=len(x))\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(x, y_exp, alpha=0.4)\n",
    "plt.title(\"Ejemplo exponencial: crecimiento acelerado\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5385d57a",
   "metadata": {},
   "source": [
    "---\n",
    "## 9) Árboles de decisión para regresión\n",
    "\n",
    "### 1️⃣ Definición real  \n",
    "Un **árbol de regresión** divide el espacio de variables en regiones (por reglas tipo “si x > 7”) y predice un valor constante (promedio) en cada región.\n",
    "\n",
    "### 2️⃣ Lógica de splits  \n",
    "El árbol elige cortes que reducen el MSE dentro de cada grupo.\n",
    "\n",
    "### 3️⃣ Detalle\n",
    "- No asume forma (ni recta ni curva).  \n",
    "- Captura relaciones escalonadas o con umbrales.  \n",
    "- Riesgo: sobreajuste si el árbol es muy profundo.\n",
    "\n",
    "### 4️⃣ Ejemplo con manzanitas  \n",
    "Precio de ticket de avión:\n",
    "- ¿temporada alta?  \n",
    "- ¿faltan menos de 7 días?  \n",
    "- ¿fin de semana?  \n",
    "La combinación te lleva a un “precio promedio” de ese segmento.\n",
    "\n",
    "### 5️⃣ Industria  \n",
    "Pricing, estimación de demanda, base de Random Forest / XGBoost / LightGBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c2ba3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Código + gráfico: árbol en datos con U (verás predicción escalonada)\n",
    "tree = DecisionTreeRegressor(max_depth=3, random_state=42)\n",
    "tree.fit(X_u, y_u)\n",
    "y_tree = tree.predict(X_u)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X_u, y_u, alpha=0.4, label=\"Datos\")\n",
    "plt.plot(X_u, y_tree, label=\"Árbol (max_depth=3)\")\n",
    "plt.title(\"Árbol de regresión: predicción por tramos\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"R2 árbol:\", r2_score(y_u, y_tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2582d331",
   "metadata": {},
   "source": [
    "---\n",
    "# PARTE 3 — EL MOTOR: OPTIMIZACIÓN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d518ac53",
   "metadata": {},
   "source": [
    "## 10) Gradiente descendente + tasa de aprendizaje\n",
    "\n",
    "### 1️⃣ Definición real  \n",
    "Es un algoritmo iterativo para minimizar una función. Se usa cuando no quieres/puedes resolver “de una” o cuando el dataset es enorme.\n",
    "\n",
    "### 2️⃣ Fórmula  \n",
    "\\[\n",
    "\\beta_{t+1}=\\beta_t-\\eta \\nabla J(\\beta)\n",
    "\\]\n",
    "\n",
    "### 3️⃣ Detalle\n",
    "- \\(\\nabla J\\): dirección de máxima subida (si restas, bajas).  \n",
    "- \\(\\eta\\): tamaño del paso (**learning rate**).  \n",
    "  - muy pequeño → tarda mucho  \n",
    "  - muy grande → rebota y puede divergir  \n",
    "- En regresión lineal, el costo es convexo → no hay mínimos locales “trampa”.\n",
    "\n",
    "### 4️⃣ Ejemplo con manzanitas  \n",
    "Buscar el fondo de un tazón a oscuras: sientes la pendiente y das un paso hacia abajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db77d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Código: gradiente descendente en una función simple (beta-4)^2\n",
    "def gradient_descent(beta0, eta, steps=20):\n",
    "    beta = beta0\n",
    "    history = []\n",
    "    for t in range(steps):\n",
    "        grad = 2*(beta - 4)      # derivada de (beta-4)^2\n",
    "        beta = beta - eta*grad\n",
    "        history.append(beta)\n",
    "    return np.array(history)\n",
    "\n",
    "hist_slow = gradient_descent(beta0=0, eta=0.05, steps=30)\n",
    "hist_good = gradient_descent(beta0=0, eta=0.2,  steps=30)\n",
    "hist_bad  = gradient_descent(beta0=0, eta=0.9,  steps=10)\n",
    "\n",
    "print(\"Último beta (eta=0.05):\", hist_slow[-1])\n",
    "print(\"Último beta (eta=0.2): \", hist_good[-1])\n",
    "print(\"Historial (eta=0.9):   \", hist_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c40fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Gráfico: cómo cambia beta con diferentes learning rates\n",
    "plt.figure()\n",
    "plt.plot(hist_slow, marker=\"o\", label=\"eta=0.05 (lento)\")\n",
    "plt.plot(hist_good, marker=\"o\", label=\"eta=0.2 (bien)\")\n",
    "plt.title(\"Gradiente descendente: efecto del learning rate\")\n",
    "plt.xlabel(\"Iteración\")\n",
    "plt.ylabel(\"beta\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188eea71",
   "metadata": {},
   "source": [
    "---\n",
    "# 11) Comparación final en un dataset “de juguete” (pero realista)\n",
    "\n",
    "Aquí hacemos lo que suele pedir la industria:  \n",
    "- separar train/test  \n",
    "- entrenar varios modelos  \n",
    "- comparar métricas  \n",
    "- comparar interpretabilidad vs performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd21451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Dataset sintético realista: y depende de 2 variables + ruido + variables irrelevantes\n",
    "n = 800\n",
    "p = 10\n",
    "X_all = np.random.normal(size=(n, p))\n",
    "\n",
    "beta_true = np.array([2.5, -1.7] + [0]*(p-2))\n",
    "y_all = X_all @ beta_true + np.random.normal(scale=1.5, size=n)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.25, random_state=42)\n",
    "\n",
    "models = {\n",
    "    \"OLS\": LinearRegression(),\n",
    "    \"Ridge(alpha=10)\": Ridge(alpha=10.0),\n",
    "    \"Lasso(alpha=0.1)\": Lasso(alpha=0.1, max_iter=10000)\n",
    "}\n",
    "\n",
    "rows = []\n",
    "for name, m in models.items():\n",
    "    m.fit(X_train, y_train)\n",
    "    pred = m.predict(X_test)\n",
    "    rows.append({\n",
    "        \"modelo\": name,\n",
    "        \"MAE\": mean_absolute_error(y_test, pred),\n",
    "        \"RMSE\": np.sqrt(mean_squared_error(y_test, pred)),\n",
    "        \"R2\": r2_score(y_test, pred),\n",
    "        \"n_coef_~0\": int(np.sum(np.isclose(getattr(m, \"coef_\", np.array([])), 0.0, atol=1e-6)))\n",
    "    })\n",
    "\n",
    "pd.DataFrame(rows).sort_values(\"RMSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac9be07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Gráfico: coeficientes aprendidos (¿quién hace selección?)\n",
    "coef_df = pd.DataFrame({\n",
    "    \"true\": beta_true,\n",
    "    \"OLS\": models[\"OLS\"].coef_,\n",
    "    \"Ridge\": models[\"Ridge(alpha=10)\"].coef_,\n",
    "    \"Lasso\": models[\"Lasso(alpha=0.1)\"].coef_\n",
    "})\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(coef_df[\"true\"].values, marker=\"o\", label=\"true\")\n",
    "plt.plot(coef_df[\"OLS\"].values, marker=\"o\", label=\"OLS\")\n",
    "plt.plot(coef_df[\"Ridge\"].values, marker=\"o\", label=\"Ridge\")\n",
    "plt.plot(coef_df[\"Lasso\"].values, marker=\"o\", label=\"Lasso\")\n",
    "plt.title(\"Coeficientes: verdad vs modelos\")\n",
    "plt.xlabel(\"Índice de variable\")\n",
    "plt.ylabel(\"Coeficiente\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "coef_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41ec68d",
   "metadata": {},
   "source": [
    "---\n",
    "# 12) Guía de decisión (industrial)\n",
    "\n",
    "**¿Qué usar y cuándo?**\n",
    "- **OLS**: baseline, máxima interpretabilidad, pero sensible a multicolinealidad.  \n",
    "- **Ridge**: muchas variables correlacionadas, quieres estabilidad.  \n",
    "- **Lasso**: quieres reducir variables (feature selection), bajar complejidad.  \n",
    "- **Polinomial**: relación curva (U/S) con sentido físico/económico.  \n",
    "- **Árboles**: reglas/umbrales, datos con saltos, poca suposición de forma.  \n",
    "- **Gradiente descendente**: dataset enorme / modelos más complejos; base de deep learning.\n",
    "\n",
    "**Pregunta final para discusión:**  \n",
    "> Si tienes 300 variables, muchas correlacionadas, y debes explicar el modelo al regulador: ¿Ridge o Árbol? ¿por qué?"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
